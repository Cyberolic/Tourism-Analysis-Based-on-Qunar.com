{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **基于博客标题文本信息的K-means 聚类**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jieba分词 精确模式\n",
    "def get_jiebaword():\n",
    "\t# enconding视文本保存的编码而定，utf-8或gbk\n",
    "    try:\n",
    "        with open('happy.txt', \"r\", encoding='utf-8',errors='ignore') as fr:\n",
    "            lines = fr.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(\"no file like this\")\n",
    "    jiebaword = []\n",
    "    for line in lines:\n",
    "        #line=re.sub('(\\:.*?\\:)', '', emoji.demojize(line))#清除标题中的emoji\n",
    "        line=re.sub(u\"([^\\u4e00-\\u9fa5])\", '', line)#直接保留中文的Unicode编码\n",
    "        line = line.strip('\\n')\n",
    "        # 清除多余的空格\n",
    "        line = \"\".join(line.split())\n",
    "        # 默认精确模式\n",
    "        seg_list = jieba.cut(line, cut_all=False)\n",
    "        word = \"/\".join(seg_list)\n",
    "        jiebaword.append(word)\n",
    "    return jiebaword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\CYBERO~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.493 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "a=get_jiebaword()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取停用词表\n",
    "def get_stopword():\n",
    "    stopword = []\n",
    "    try:\n",
    "        with open('C:/Users/Cyberolic/Desktop/stopwords.txt', \"r\", encoding='utf-8') as fr:\n",
    "            lines = fr.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(\"no file like this\")\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n')\n",
    "        stopword.append(line)\n",
    "    return stopword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可以不加\n",
    "b=get_stopword()\n",
    "b.append('｜')\n",
    "b.append('～')\n",
    "b.append('『')\n",
    "b.append('”')\n",
    "b.append('』')\n",
    "b.append('“')\n",
    "b.append('❀')\n",
    "b.append('℃')\n",
    "b.append('❥')\n",
    "b.append('°')\n",
    "b.append('一')\n",
    "b.append('の')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除停用词\n",
    "def clean_stopword(jiebaword,stopword):\n",
    "    fw = open('CleanWords.txt', 'a+',encoding='utf-8')\n",
    "    for words in jiebaword:\n",
    "        words = words.split('/')\n",
    "        for word in words:\n",
    "            if word not in stopword:\n",
    "                fw.write(word + '\\t')\n",
    "        fw.write('\\n')\n",
    "    fw.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_stopword(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成tf-idf矩阵文档\n",
    "def get_tfidf():\n",
    "    try:\n",
    "        with open('CleanWords.txt', \"r\", encoding='utf-8') as fr:\n",
    "            lines = fr.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(\"no file like this\")\n",
    "    transformer=TfidfVectorizer()\n",
    "    tfidf = transformer.fit_transform(lines)\n",
    "    # 转为数组形式\n",
    "    tfidf_arr = tfidf.toarray()\n",
    "    return tfidf_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(1568, 3799)\n"
     ]
    }
   ],
   "source": [
    "tfidf_arr=get_tfidf()\n",
    "print(tfidf_arr)\n",
    "print(tfidf_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer, cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means聚类\n",
    "def get_cluster(tfidf_arr,k):\n",
    "    kmeans = KMeansClusterer(num_means=k, distance=cosine_distance)  # 分成k类，使用余弦相似分析\n",
    "    kmeans.cluster(tfidf_arr)\n",
    "    # 获取分类\n",
    "    kinds = pd.Series([kmeans.classify(i) for i in tfidf_arr])\n",
    "    fw = open('ClusterText.txt', 'a+', encoding='utf-8')\n",
    "    for i, v in kinds.items():\n",
    "        fw.write(str(i) + '\\t' + str(v) + '\\n')\n",
    "    fw.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cluster(tfidf_arr,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取分类文档\n",
    "def cluster_text():\n",
    "    index_cluser = []\n",
    "    try:\n",
    "        with open('ClusterText.txt', \"r\", encoding='utf-8') as fr:\n",
    "            lines = fr.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(\"no file like this\")\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n')\n",
    "        line = line.split('\\t')\n",
    "        index_cluser.append(line)\n",
    "    # index_cluser[i][j]表示第i行第j列\n",
    "    try:\n",
    "        with open('CleanWords.txt', \"r\", encoding='utf-8') as fr:\n",
    "            lines = fr.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(\"no file like this\")\n",
    "    for index,line in enumerate(lines):\n",
    "        for i in range(1567):\n",
    "            if str(index) == index_cluser[i][0]:\n",
    "                fw = open('cluster' + index_cluser[i][1] + '.txt', 'a+', encoding='utf-8')\n",
    "                fw.write(line)\n",
    "    fw.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取主题词\n",
    "def get_title(cluster):\n",
    "    for i in range(cluster):\n",
    "        try:\n",
    "            with open('cluster' + str(i) + '.txt', \"r\", encoding='utf-8') as fr:\n",
    "                lines = fr.readlines()\n",
    "        except FileNotFoundError:\n",
    "            print(\"no file like this\")\n",
    "        all_words = []\n",
    "        for line in lines:\n",
    "            line = line.strip('\\n')\n",
    "            line = line.split('\\t')\n",
    "            for word in line:\n",
    "                all_words.append(word)\n",
    "        c = Counter()\n",
    "        for x in all_words:\n",
    "            if len(x) > 1 and x != '\\r\\n':\n",
    "                c[x] += 1\n",
    "\n",
    "        print('主题' + str(i+1) + '\\n词频统计结果：')\n",
    "        # 输出词频最高的那个词，也可以输出多个高频词\n",
    "        for (k, v) in c.most_common(1):  \n",
    "            print(k,':',v,'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题1\n",
      "词频统计结果：\n",
      "攻略 : 82 \n",
      "\n",
      "主题2\n",
      "词频统计结果：\n",
      "三亚 : 129 \n",
      "\n",
      "主题3\n",
      "词频统计结果：\n",
      "自驾 : 113 \n",
      "\n",
      "主题4\n",
      "词频统计结果：\n",
      "周末 : 20 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_title(4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e1489fd42517f67afba4305207d21285b867f48f34b89b5d0face3074316e5d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
